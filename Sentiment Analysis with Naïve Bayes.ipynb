{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8aa2ae6",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Na√Øve Bayes\n",
    "Concepts:\n",
    "-  Run a simple implementation of a Na√Øve Bayes spam filter by hand\n",
    "-  Run NLTK's implementation of a Na√Øve Bayes classifier to sort movie reviews into positive and negative\n",
    "-  Carry out feature engineering to improve the performance of a binary classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2e768d",
   "metadata": {},
   "source": [
    "## 1. Na√Øve Bayes spam filtering by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9aca2b",
   "metadata": {},
   "source": [
    "### 1.2. Set up the spreadsheet\n",
    "This section will be done with the following [spreadsheet](https://docs.google.com/spreadsheets/d/1Ipd_qC_2R7FZEmkuPFZ_u3Jxn_MmFMGMzuz1zLQTKBQ/edit?usp=sharing).\n",
    "\n",
    "The Na√Øve Bayes approach to spam filtering is based on how often certain words occur in spam emails and in non-spam emails. For example, the word \"Viagra\" often occurs in spam emails and rarely occurs in non-spam emails. For these two reasons, the occurrence of this word is a reliable indicator of spam. We can estimate the reliability of any word-based feature in the following way:\n",
    "- We count the number of spam emails in our training set that contain the word, and divide it by the number of spam emails in our training set overall. The higher this number, the stronger this word provides evidence into the \"spam\" direction. \n",
    "- We also count the number of non-spam emails in our training set that contain this word, and divide this by the number of non-spam emails in our training set overall. The higher this number, the stronger this word provides evidence into the \"non-spam\" direction. \n",
    "- We then set up a tug-of-war between the two numbers by dividing the first number by the second. The result is called the odds ratio. It will be close to 1 if the word occurs about equally often in spam and non-spam emails. The way we set it up in this exercise, the odds ratio will be above 1 (tending towards infinity) if the word occurs predominantly in spam emails, and below 1 (tending towards zero) if the word occurs predominantly in non-spam emails. We could also have set it up the other way around, so that odds ratios above 1 indicate ham and odds rations below 1 indicate spam, and it would have worked out just fine -- the choice is arbitrary here. As long as the choice is made in a consistent way throughout an application, it will not matter.\n",
    "\n",
    "One problem with this and similar approaches arises from the fact that certain words will not occur in any non-spam emails in the test set. For example, \"Viagra\" occurs so rarely in non-spam emails that in many cases, a training set will not contain it at all. This is the case in our example. As a result, computing the odds ratio as described above results in division by zero.\n",
    "\n",
    "After studying this problem carefully for a few thousands of years üòä, a task force of machine learning experts has come up with a clever solution: simply add one to every count. This is called \"add-one smoothing\". This way, every number is guaranteed to be positive and there is no risk of running into division by zero. Mathematically, this corresponds to taking the entire list of known words and pretending that it occurs twice in the training set, once as a spam email and once as a non-spam email. Since we add it on both sides of the divide, we don't change anything about how the individual words are categorized: every feature that used to pull us into a certain direction will still do so. \n",
    "\n",
    "In the spreadsheet, columns B and C give the counts of particular words found in spam and ham emails for a hypothetical user named Sandy, who chats about horses. The next two columns, D and E, are intended to hold smoothed counts, i.e. ones with no zeros. However, the counts initially are the same as those in columns B and C (i.e., they are initially unsmoothed). Consequently, there are divide-by-zero errors in columns H and J."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b443e2b5",
   "metadata": {},
   "source": [
    "### 1.2. Complete the spreadsheet\n",
    "The second step is to smooth the counts in columns D and E. We‚Äôre going to use the simple add-one smoothing method. Select cell D5, which should have the formula `=B5`, and replace it with the formula `=B5+1`, thereby adding one to the count. \n",
    "\n",
    "Next, add one to the count of all the words. To do so, select cell D5 again, then press command-C on Macs or Ctrl-C on Windows (or select Edit|Copy), then select the cells for all the words in columns D and E (i.e., cells `D5:E15`) and press command-V (or Ctrl-V, or select Edit|Paste).\n",
    "\n",
    "By smoothing the counts, the smoothed probabilities in columns F and G should all become non-zero. These probabilities are simply the relative frequencies of the counts in columns D and E, i.e. the count of each word divided by the total. \n",
    "\n",
    "Column H has the ratio of the probabilities in columns D and E, which gives the odds ratio of a message being spam rather than ham just based on each word. \n",
    "\n",
    "Column I has the counts of each word for Message 1; for example, the word *cash* appears once in Message 1 (note that we don‚Äôt have data for all the words in all the messages ‚Äî we‚Äôll assume that the other words are inconsequential). \n",
    "\n",
    "Column J carries over the relevant ratios: for words with non-zero counts, it has the product of the counts in column I times the ratios in column H. For words with zero counts (words that don't occur in the message), it has 1.\n",
    "\n",
    "Finally, cell J16 has the product of the ratios in Column J, which gives the overall odds ratio for the message being spam, while cell J17 has the inverse of J16, which is the odds ratio for the message being ham. This is how a simple spam filter works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca731b5",
   "metadata": {},
   "source": [
    "### 1.3. Interpret the spreadsheet\n",
    "Message 1 is classified as spam since the overall odds ratio (J16 = 225.368) is above 1 and approaches infinity. Its inverse overall odds ratio (J17 = 0.004) also approaches 0, which means its words occur predominantly in spam emails.\n",
    "\n",
    "Message 2 is classified as non-spam since the overall odds ratio (L16 = 0.000) is below 1 and approaches 0. Its inverse overall odds ratio (L17 = 2871.649) is also really big and approaches infinity, which means its words occur predominantly in non-spam emails.\n",
    "\n",
    "Message 3 is classified as non-spam since the overall odds ratio (N16 = 0.04) is below 1 and approaches 0. Its inverse overall odds ratio (N17 = 24.896) is also really big and approaches infinity, which means its words occur predominantly in non-spam emails. This goes as expected because the words that appear in Message 3 have low ratios which makes the overall odds ratio approach 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16cedb2",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Analyze movie reviews with a Na√Øve Bayes classifier\n",
    "In this section, a Na√Øve Bayes classifier will be used to analyze movie reviews. This will require a couple of assumptions: \n",
    "\n",
    "- First, we're assuming that every review can be classified as either positive or negative (no neutral reviews).\n",
    "- Second, documents can be represented as a bag of words, with sequential order being unimportant.\n",
    "- Third, the probabilities of two words $x$ and $y$ appearing in a document are independent (this is the *na√Øve* part of Na√Øve Bayes). This is saying that the probability of encountering the word \"butter\" is unchanged by seeing the word \"peanut\" in the same sentence, even though we generally have the intuition that $P(\\text{butter}|\\text{peanut})$ is higher than, for example $P(\\text{butter}|\\text{supernova})$.\n",
    "\n",
    "We know that assumptions 2 and 3 are wrong, but they make things much simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4d7714",
   "metadata": {},
   "source": [
    "### 2.1. Training Data\n",
    "\n",
    "The first thing needed for a classifier is a set of labeled reviews to use to train the model. NLTK has a corpus of 2000 labeled movie reviews that should work well for this. Run the cell below to import and transform the reviews into a convenient form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06063d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're taking the corpus of movie reviews and transforming them into a useful form\n",
    "\n",
    "import nltk\n",
    "import random\n",
    "import re\n",
    "from nltk.corpus import movie_reviews\n",
    "from pathlib import Path\n",
    "from os.path import exists\n",
    "\n",
    "common_nltk_data_location = str(Path.home())+'/groupshare/nltk_data'\n",
    "if exists(common_nltk_data_location):\n",
    "    nltk.data.path=[common_nltk_data_location]\n",
    "else:\n",
    "    nltk.download('punkt') \n",
    "    nltk.download('movie_reviews')  \n",
    "movies = [(list(movie_reviews.words(fileid)),category) for category in movie_reviews.categories() for fileid in movie_reviews.fileids(category)]\n",
    "random.seed(8823)\n",
    "random.shuffle(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "938168c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review:\n",
      "delicatessen (directors: marc caro/jean-pierre jeunet; screenwriters: gilles adrien/marc caro; cinematographer: darius khondji; editor: herve schneid; cast: dominique pinon (louison), marie-laure dougnac (julie clapet), jean-claude dreyfus (clapet-the butcher), karin viard (mademoiselle plusse), ticky holgado (marcel tapioca), anne-marie pisani (madame tapioca), jacques mathou (roger), rufus (robert kube), howard vernon (frog man), edith ker (granny), boban janevski (young rascal), mikael todde (young rascal), chick ortega (postman), silvie laguna (aurore interligator), howard vernon (frog man); runtime: 96; miramax/constellation/ugc/hatchette premiere; 1991-france) reviewed by dennis schwartz a black comedy set in the near future in a boarding house run by a depraved butcher.\n",
      "\n",
      "the comedy is played more in comic strip style for entertaining value than for deeper satire, as it features mostly zany sophomoric sight gags and relies heavily on special effects.\n",
      "\n",
      "the world has fallen on hard times and there are food shortages which include no meat, so the butcher serves up meat from human flesh to customers who pay with grain, almost as valued a commodity.\n",
      "\n",
      "that' s the big joke in the film and the novelty of that cannibalism idea wears thin mighty fast, as the characters are too absurd and sketched too thinly for us to care about them.\n",
      "\n",
      "this tasteless postapocalyptic french comedy is a first feature for the co-directors marc caro/jean-pierre jeunet.\n",
      "\n",
      "it failed to reach my funny bone and instead left me mostly annoyed at its slight story and its dark projections for the future.\n",
      "\n",
      "an ex-circus clown named louison (dominique pinon), the film' s too-good-to-be-true hero, answers an ad for work as a handyman for clapet (jean-claude dreyfus), and the butcher and landlord, offers him room and board in his house.\n",
      "\n",
      "the butcher' s clumsy and near-sighted daughter julie (marie-laure dougnac) falls in love with the skinny, weird looking clown, and the two make some music together, with her playing the cello and him a saw.\n",
      "\n",
      "they are the innocents, surrounded by a boarding house of misfits suffering from fear and watched over by her overbearing father, who has lured into his tenement the clown, as he has his past innocent victims, so that he can put his cleaver to him and then sell him as meat, which he intends to do as soon as the clown fixes up the tenement.\n",
      "\n",
      "the entire film takes place in the shabby tenement, and the tenants are an odd lot of bizarre malcontents, who do not trust each other.\n",
      "\n",
      "there are two youngsters (boban janevski & mikael todde) who do any kind of mischief they can.\n",
      "\n",
      "a frog man (howard vernon) who lives with water on the floor so he can raise his frogs and snails that he eats.\n",
      "\n",
      "two brothers (mathou & kube) who create little cow-moo novelty toys.\n",
      "\n",
      "a man (holgado) who sells a bullshit detector to the butcher for his piece of meat.\n",
      "\n",
      "a slutty woman (karin viard), who lives with the butcher and only wants his meat.\n",
      "\n",
      "the aristocratic woman (silvie laguna) who tries numerous times to commit suicide but is too inept to do it right.\n",
      "\n",
      "the tenants are too afraid to come out at night because they know what the butcher is up to, so they are forced to communicate with each other through a pipe that runs through the building (in one scene they are all in musical harmony to the lovemaking of the butcher and his gal, as their bedsprings squeak).\n",
      "\n",
      "there is also a sex-crazed postman (chick ortega), who lusts for the butcher' s daughter and carries a gun while delivering the mail.\n",
      "\n",
      "there is also an underdeveloped subplot about a band of incompetent underground veggie fanatics, called trogolodistes, who have been summoned to rescue the clown and steal some grain.\n",
      "\n",
      "the directors overloaded the film with too many eccentrics, as the comedy seemed forced while the surreal look of the film added no dramatic intensity.\n",
      "\n",
      "delicatessen could have some appeal to the cult film crowd who like their meat sliced thin, monty python fans, and those who liked terry gilliam' s brazil, a film similar in spirit.\n",
      "-------------------------\n",
      "label: neg\n"
     ]
    }
   ],
   "source": [
    "# each item in the list has the tuple structure (review,label) where review is a list of words and label is a string that either says 'pos' or 'neg' (the u before each string indicates that it's unicode)\n",
    "\n",
    "print(\"Review:\")\n",
    "review = ' '.join(movies[0][0])\n",
    "review = review.replace(\". \", \".\\n\\n\") # adds newlines after sentences\n",
    "review = review.replace(\"( \", \"(\").replace(\" )\", \")\") # remove whitespace after opening paren and before closing paren\n",
    "review = review.replace(\" - \", \"-\").replace(\" / \", \"/\") # remove surrounding whitespace around - and /\n",
    "review = re.sub(r'\\s([?.!\",\\';:‚Äò](?:\\s|$))', r'\\1', review) # removes whitespaces before punctuation\n",
    "print(review)\n",
    "print(\"-------------------------\")\n",
    "print(\"label:\", movies[0][1]) # print the above review's label (pos or neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebf6fe8",
   "metadata": {},
   "source": [
    "### 2.2. Picking initial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a16bc3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 77717), ('the', 76529), ('.', 65876), ('a', 38106), ('and', 35576), ('of', 34123), ('to', 31937), (\"'\", 30585), ('is', 25195), ('in', 21822), ('s', 18513), ('\"', 17612), ('it', 16107), ('that', 15924), ('-', 15595), (')', 11781), ('(', 11664), ('as', 11378), ('with', 10792), ('for', 9961), ('his', 9587), ('this', 9578), ('film', 9517), ('i', 8889), ('he', 8864), ('but', 8634), ('on', 7385), ('are', 6949), ('t', 6410), ('by', 6261), ('be', 6174), ('one', 5852), ('movie', 5771), ('an', 5744), ('who', 5692), ('not', 5577), ('you', 5316), ('from', 4999), ('at', 4986), ('was', 4940), ('have', 4901), ('they', 4825), ('has', 4719), ('her', 4522), ('all', 4373), ('?', 3771), ('there', 3770), ('like', 3690), ('so', 3683), ('out', 3637), ('about', 3523), ('up', 3405), ('more', 3347), ('what', 3322), ('when', 3258), ('which', 3161), ('or', 3148), ('she', 3141), ('their', 3122), (':', 3042), ('some', 2985), ('just', 2905), ('can', 2882), ('if', 2799), ('we', 2775), ('him', 2633), ('into', 2623), ('even', 2565), ('only', 2495), ('than', 2474), ('no', 2472), ('good', 2411), ('time', 2411), ('most', 2306), ('its', 2270), ('will', 2216), ('story', 2169), ('would', 2109), ('been', 2050), ('much', 2049), ('character', 2020), ('also', 1967), ('get', 1949), ('other', 1948), ('do', 1915), ('two', 1911), ('well', 1906), ('them', 1877), ('very', 1863), ('characters', 1859), (';', 1850), ('first', 1836), ('--', 1815), ('after', 1762), ('see', 1749), ('!', 1713), ('way', 1693), ('because', 1684), ('make', 1642), ('life', 1586)]\n"
     ]
    }
   ],
   "source": [
    "# find the 100 most common words in the reviews\n",
    "\n",
    "frequencies = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
    "most_common = frequencies.most_common(100) \n",
    "print(most_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cae1477",
   "metadata": {},
   "source": [
    "### 2.3. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94607efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick some random set of features/words to start with\n",
    "features = [\"the\",\"only\",\"old\",\"almost\",\"good\",\"bad\"]\n",
    "\n",
    "# check if review contains selected features\n",
    "def extract_features(review,features):\n",
    "    doc_features = {}\n",
    "    for word in features:\n",
    "        doc_features[word] = (word in review)\n",
    "    return doc_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d647eea",
   "metadata": {},
   "source": [
    "We don't actually care about the movie reviews themselves but only about the features they contain, together with the information about whether the reviews are positive or negative. With the feature extractor function just defined, we can turn all of the movie reviews into feature vectors. The feature vector of a given review consists of our word list together with information about whether the review contains each of these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91d13dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "({'the': True, 'only': True, 'old': False, 'almost': True, 'good': True, 'bad': False}, 'neg')\n"
     ]
    }
   ],
   "source": [
    "# outputs a list of features together with their values (True means that the word occurs in the review, False means that it doesn't) together with a label (pos means that the review is positive, neg means it is negative)\n",
    "movie_features = [(extract_features(review,features),category) for (review,category) in movies]\n",
    "print(len(movie_features))\n",
    "print(movie_features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0cace7",
   "metadata": {},
   "source": [
    "### 2.4. Training the Model\n",
    "The NLTK NaiveBayesClassifier() function can now be trained on the feature vectors we have just extracted. To do this, we arbitrarily split our list of movie review feature vectors into training data (let's say reviews 100 and up) and test data (reviews 0-99). We hand the training data to the classifier function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8224b47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and testing sets:\n",
    "movies_training, movies_test = movie_features[100:], movie_features[:100]\n",
    "our_first_classifier = nltk.NaiveBayesClassifier.train(movies_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b440cac6",
   "metadata": {},
   "source": [
    "Now we can see how well our classifier has learned to distinguish between positive and negative reviews. To put the numbers you are about to see in perspective, let's first consider the baseline of this task, that is, how well we would expect to do if we classified reviews 0-99 as positive or negative completely blindly, without looking at their contents. We would not want to pay any money for a classifier that doesn't do much better than this baseline. A quick check tells us that there are almost equally many positive and negative movie reviews in our test set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f1ec8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive reviews in our test set: 49\n",
      "Number of negative reviews in our test set: 51\n"
     ]
    }
   ],
   "source": [
    "count_pos = 0\n",
    "count_neg = 0\n",
    "for i in range(0, 100):\n",
    "    label = movies[i][1]\n",
    "    if label == \"pos\":\n",
    "        count_pos += 1\n",
    "    elif label == \"neg\":\n",
    "        count_neg += 1\n",
    "print(\"Number of positive reviews in our test set:\", count_pos)\n",
    "print(\"Number of negative reviews in our test set:\", count_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c2dd84",
   "metadata": {},
   "source": [
    "So, we can expect that if we just flipped a coin, we would be right about half the time. In other words, the baseline for this task is about 50%. (If we want to be pedantic, we can consider 51% to be the baseline, since an extremely pessimistic classifier would reach 51% accuracy by classifying every review in the test set as negative without bothering to read it.)\n",
    "\n",
    "We will first try out how well it fares on the reviews that it has just used to learn that difference, that is, the training set (movies_training). Run the cell below to tell our classifier to classify the training set and check how well it did. The result is a fraction between 0 and 1, so the code multiplies it with 100 to get a percentage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "285bd4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the first classifier on reviews 100-2000 from the NLTK corpus (previously used as training data):\n",
      "62.37%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the first classifier on reviews 100-2000 from the NLTK corpus (previously used as training data):\")\n",
    "print(str(round(100 * nltk.classify.accuracy(our_first_classifier,movies_training),2)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8c271e",
   "metadata": {},
   "source": [
    "Running a classifier on its own training set is not meaningful except to tell us that it is not completely blind. It would not be good practice to use the resulting accuracy as a way to evaluate the classifier's performance, because it doesn't tell us whether the classifier is able to generalize beyond what it has already seen. The more meaningful result will come from using data on which the classifier has not been trained.  The cell below tries out our classifier on a test set which contains entirely new data (the reviews 0-99 that we have previously set aside for this purpose). We will use this as a baseline to compare with a new classifier with new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6bea84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the first classifier on reviews 0-99 from the NLTK corpus (baseline):\n",
      "71.0%\n"
     ]
    }
   ],
   "source": [
    "baseline = nltk.classify.accuracy(our_first_classifier,movies_test)\n",
    "print(\"Accuracy of the first classifier on reviews 0-99 from the NLTK corpus (baseline):\")\n",
    "print(str(round(100 * baseline ,2)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e13232b",
   "metadata": {},
   "source": [
    "### 2.5. Applying the Model to New Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80fa5fb",
   "metadata": {},
   "source": [
    "Now let's see how well the trained classifier does with a set of entirely new movie reviews. Let's run the classifier on the reviews below, and then calculate its precision (positive predictive value) and recall (true positive rate or sensitivity). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a97b63a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import new movie reviews (from Rotten Tomatoes, various movies)\n",
    "test1 = \"After the pleasant surprise that was the first film of the new Planet of the Apes series, the expectations for the sequel, or middle part of the trilogy, were somewhat bigger. Thankfully, everyone involved was fully aware of that and delivered another smart blockbuster with a lot of vital commentary on the futility of war and violent conflicts. The film doesn't want you to pick a side too easily as hostility between the last remaining humans on Earth and the intelligent apes arise. There are decent and bad characters on both sides. This makes for an interesting ride, as the conflict spins more and more into chaos and there is little anyone can do against it, after a point of no return. Once again, the CGI is incredible, thanks to great motion capture acting and the accompanying special effects. Thankfully, the human actors are en par, especially Gary Oldman only takes two short scenes to make a strong point for being one of the best of his generation. The gloomy atmosphere, the great cinematography, it all adds up to an intelligent and pretty damn entertaining continuation of the story. If there is one complaint it would have to be that the ending is merely a cliffhanger for what's next in part three. But at least we all have something to look forward to.\" # positive\n",
    "\n",
    "test2 = \"I will start off this review with a caveat that I am not the biggest fan of Michael Bay films, its not that I don't like any of his films but i am just not the biggest fan. This is a Michael Bay film from beginning to end. For people that like Michael Bay's style and the other films in this franchise will certainly love this film. I actually did enjoy the first film in this franchise but every installment after has been worse. The first criticism is that Michael Bay's directing style and cliches were so heavy handed in this film that it became its own character and became a spoof of itself, it took me out of the film; from slow motion sequences, to low camera angles, and one liners that did not quite hit hard enough. The product placement in this film was also very in your face and often took me out of the film. The dialogue was cringe worthy and I often felt like Peter Cullen (voice of Optimus Prime) did not want to say half the lines. The plot was extremely convoluted partly due to this movie being just way too long (nearly 3 hours). There is one positive for this film though and it barely counts. Even if your not a fan of Michael Bay, you can never argue with the amazing visuals and intense action sequences that he brings to the screen though after a while of things just blowing up I began to get bored. Overall this film is is a steaming pile of crap and for people who are not me you need to be big fan of Michael Bay and other films in this franchise to truly enjoy it. Though even if you are a fan of Michael Bay it is going to be hard to enjoy this film as it is one of the worst movies I have ever seen.\" # negative\n",
    "\n",
    "test3 = \"Both leads are playing their stereotypical roles, but they feel very comfortable in it. Really the best part of this film is watching these two actors go head to head in some really good scenes. Aside from those few scenes though, most of the film is so schmaltzy and predictable that it doesn't make sense for the film to be as long as it is. The direction is so flawed in its portrayal of several characters and too misguided in others that its hard to take many performances seriously. There's an entire sub plot with Farmiga that could have been completely removed, and there is a brother with a disability character who borders on offensive for much of the film. Billy Bob Thornton is so underutilized in this film that I don't know why he signed on, and the same is true for Vincent D'Onofrio. I don't know why the film is as long as it is, and it feels so self-indulgent for the director most of the time. If it weren't for Robert Downey Jr. and Duvall's performances, this film wouldn't have almost anything going for it. I was surprised that the courtroom scenes were as lackluster as they were, I was really expecting to enjoy those. They just fell flat most of the time. It's overall inoffensive, but it is nothing spectacular.\" # negative\n",
    "\n",
    "test4 = \"David Ayer, fresh off of a weird mixture of directing \\\"Sabotage\\\" and \\\"End of Watch\\\" (which he also wrote and produced), \\\"Fury\\\" could have gone either way, but I must say, this film is extremely impressive. Every crew member aboard that tank gives it their all in their performances and that is definitely the dividing line between whether or not this film would be good or bad. Brad Pitt, Logan Lerman, Shia Labeouf, John Bernthal, and Michael Pena are all believable in their roles. Normally I wouldn't waste my time listing every cast member, but there is not one bad performance here and everyone deserves recognition for their work. Yes, a few of them are a little underdeveloped, but you understand theirt motivations and pride for their country the whole way through. I wa immersed in these immaculatly shot war sequences that will have your heart pumping. It has been a while that I was so immersed in a film like I was with \\\"Fury\\\" and that is saying something. The brutally honest emotions given by all the characters throughout this film are terrific and you will not even think this film is 135 Minutes long, because the experience is immersive. \\\"Fury\\\" is the best war film I have seen in a very long time. It has a few nitpicking scenes, but other than that it blew me away. \\\"Fury\\\" hit's it's target.\" # positive\n",
    "\n",
    "test5 = \"This new film fuses together everything good about the original films, as well as the recent Marvel films, and does so with gusto. There's just so much to love about this film, from the reassembled cast, to the asides for fans of the comics, to the awe inspiring action and it all works well together. This film comes on the heels of the rights transferring from Fox to Marvel, and it shows in the production value, which obviously has help from Marvel Studios, to set up for their newly announced 2016 film for the X-Men canon. It's just brilliantly constructed, bringing all your favorite characters together, while also showing new information and new characters for us to love. Most of what we see comes directly from the comics, and that's something to rejoice over, but it's also pure, perfect, psychological action thriller. This is the new breed of X-Men, and they're far more intelligent and calculated than ever before.\" # positive\n",
    "\n",
    "test6 = \"Derivative, needlessly shaky, poorly acted and devoid of excitement, Earth to Echo is an adventure film which lacks a relatively vital component; any sense of adventure. Apart from Astro's performance as Tuck, and a viscerally compelling sequence involving Reese C. Hartwig's character Munch, Earth to Echo is a film which shall displease fans of the alien-discovery category of film, as well as kids desiring a film full of varied and interesting action. Relatively impressive visual effects save the film from an entirely poor rating, though this is still surely one to miss.\" # negative\n",
    "\n",
    "test7= \"Before viewing this film, I lowered my expectations, knowing that the film was probably going to be all dick and fart jokes. Not only was that exactly what this film is, but it is also savagely racist, and Seth McFarlane's presence is very off-putting, because he is a much better voice actor. He thought he could put together a sloppy old-fashion western comedy with modern-day lingo thrown in, and normally a movie that does that is hit or miss, but this just misses almost every single time. \\\"A Million Ways To Die In The West\\\" is easily the worst comedy that has come from 2014 so far. While not being a fan of Family Guy should not affect my viewings on this film, it feels like the same stupid humour that is present there, just a lot more gross-out stuff. Don't get me wrong, I laughed at \\\"Ted\\\" as much as the next guy, but this just feels like the decline of McFarlane's career. With poor writing and sloppy directing, there is not much to like here and it will hardly gain a single laugh.\" # negative\n",
    "\n",
    "test8 = \"This movie is definitely for a more mature audience, but I give this movie a round of applause. It provides a comedic effect to serious situations of life and it also shares its awkward moments that seem to be very natural in life and that is why I give this film a high rating because it mirrors life as we know it. This indie movie was great for Jenny Slate to star in...this is good for her resume and is just good for her in general. I hope she gets many more films to come later on in her future career. She seems as if she can further develop into a multi-dimensional actress.\" # positive\n",
    "\n",
    "test9 = \"Romantic, inspiring, and strongly performed, Belle is a period piece that transcends its trappings, and becomes a film that has a lot to say about life and the way we see ourselves. Powerfully led by actress Gugu Mbatha-Raw, the entire cast of Belle finds the humanity in their characters, and every character feels like a real person. Watching Dido struggle with her self-worth and the problem of racism in the world is so captivating and enthralling that you don't want to look away. I'm not even a big fan of period-piece romances, but this film had my heart crying out for Dido to find true love. It's an incredibly sweet and earnest film, and it deserves every sweet moment it has.\" # positive\n",
    "\n",
    "test10 = \"Let's not waste too much time assessing the insipidness that contains TASM2. With very few redeeming qualities, the follow up to the Andrew Garfield starring reboot is even worse than its predecessor. What were studio heads thinking (were they even thinking). When movies are made to treat the audience as lab rats, testing to see when enough is enough, it can only spell eventual doom. This series not only condescend to its intended audience but down right insults the average viewer with continuously pretentious tongue and cheek self-aggrandizing winking. Its saving grace is its top-notch production values, sadly used to promote a frivolous film.\" # negative\n",
    "\n",
    "new_test_set = [test1,test2,test3,test4,test5,test6,test7,test8,test9,test10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93eb7b6",
   "metadata": {},
   "source": [
    "According to the original source, the ten reviews above can be categorized as follows:\n",
    "\n",
    "1 : pos\n",
    "2 : neg\n",
    "3 : neg\n",
    "4 : pos\n",
    "5 : pos\n",
    "6 : neg\n",
    "7 : neg\n",
    "8 : pos\n",
    "9 : pos\n",
    "10 : neg\n",
    "\n",
    "Without telling the classifier about these labels, store them (as \"true_labels\") to assess the classifier's performance later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b98ad5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : neg\n",
      "2 : pos\n",
      "3 : pos\n",
      "4 : neg\n",
      "5 : pos\n",
      "6 : pos\n",
      "7 : pos\n",
      "8 : neg\n",
      "9 : pos\n",
      "10 : pos\n"
     ]
    }
   ],
   "source": [
    "true_labels=['pos','neg','neg','pos','pos','neg','neg','pos','pos','neg']\n",
    "\n",
    "# run the first classifier to classify the reviews on its own\n",
    "i = 1\n",
    "for review in new_test_set:\n",
    "    print(i,':',our_first_classifier.classify(extract_features(nltk.word_tokenize(review),features)))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dee08558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the first classifier on the ten reviews given above: 20.0 %\n"
     ]
    }
   ],
   "source": [
    "# compute the first classfier's accuracy on the 10 reviews given above\n",
    "\n",
    "i=0\n",
    "correctly_classified_reviews=0\n",
    "incorrectly_classified_reviews=0\n",
    "\n",
    "# compute numbers of correctly and incorrectly classified reviews\n",
    "for review in new_test_set:\n",
    "    what_classifier_thinks = our_first_classifier.classify(extract_features(nltk.word_tokenize(review),features))\n",
    "    the_truth = true_labels[i]\n",
    "    if what_classifier_thinks == the_truth:\n",
    "        correctly_classified_reviews += 1 \n",
    "    else:\n",
    "        incorrectly_classified_reviews +=1\n",
    "    i+=1 # increase counter by 1\n",
    "\n",
    "# convert these numbers from integer to float to make division easier\n",
    "correctly_classified_reviews = float(correctly_classified_reviews)\n",
    "incorrectly_classified_reviews = float(incorrectly_classified_reviews)\n",
    "\n",
    "accuracy = correctly_classified_reviews / (correctly_classified_reviews + incorrectly_classified_reviews)\n",
    "print(\"Accuracy of the first classifier on the ten reviews given above:\", 100 * accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f4efff",
   "metadata": {},
   "source": [
    "As observed, accuracy on a small test set (Rotten Tomatoes test set) is not comparable to the results on the larger test set (NLTK test set). It is in fact much smaller. The small test set has less words that the classifier can work with, the sample size might not be representative in the first place.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a453957",
   "metadata": {},
   "source": [
    "### 2.6. Feature Engineering\n",
    "In this section, revise the feature set and train a new classifier to improve the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0c711f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                     bad = True              neg : pos    =      1.9 : 1.0\n",
      "                     bad = False             pos : neg    =      1.5 : 1.0\n",
      "                    only = False             pos : neg    =      1.2 : 1.0\n",
      "                    only = True              neg : pos    =      1.1 : 1.0\n",
      "                  almost = True              pos : neg    =      1.1 : 1.0\n",
      "                     old = True              pos : neg    =      1.1 : 1.0\n",
      "                  almost = False             neg : pos    =      1.0 : 1.0\n",
      "                    good = False             neg : pos    =      1.0 : 1.0\n",
      "                     old = False             neg : pos    =      1.0 : 1.0\n",
      "                    good = True              pos : neg    =      1.0 : 1.0\n",
      "                     the = True              pos : neg    =      1.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# show most informative features\n",
    "our_first_classifier.show_most_informative_features(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e78750",
   "metadata": {},
   "source": [
    "The features with the most extreme odds ratio are listed first. For example, something like \"good = True, pos : neg = 5.0 : 1.0\" means that based on the presence of the word \"good\" alone, the classifier places the odds of the review containing it being positive at 5 to 1. The classifier uses both the presence and the absence of a given word as a feature, so each word occurs twice in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9238cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'bad', 'fantastic', 'pleassant', 'weird', 'fan', 'poor', 'like', 'love', 'enjoy', 'worse', 'incredible', 'displease', 'top-notch']\n",
      "Accuracy of the student-defined classifier: 72.0 %\n",
      "Baseline (accuracy of the classifier we defined above): 71.0 %\n",
      "Improvement compared with baseline: 1.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['good',\n",
       " 'bad',\n",
       " 'fantastic',\n",
       " 'pleassant',\n",
       " 'weird',\n",
       " 'fan',\n",
       " 'poor',\n",
       " 'like',\n",
       " 'love',\n",
       " 'enjoy',\n",
       " 'worse',\n",
       " 'incredible',\n",
       " 'displease',\n",
       " 'top-notch']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update feature set\n",
    "student_defined_features = [\"good\", \"bad\", \"fantastic\", \"pleassant\", \"weird\", \"fan\", \"poor\", \"like\", \"love\", \"enjoy\", \"worse\", \"incredible\", \"displease\", \"top-notch\"]\n",
    "\n",
    "print(student_defined_features)\n",
    "\n",
    "# extract features\n",
    "movie_features2 = [(extract_features(review,student_defined_features),category) for (review,category) in movies]\n",
    "# split into training and testing sets\n",
    "movies_training2, movies_test2 = movie_features2[100:], movie_features2[:100]\n",
    "our_second_classifier = nltk.NaiveBayesClassifier.train(movies_training2)\n",
    "# see how well our second classifier fares on the test set\n",
    "accuracy_of_student_classifier = nltk.classify.accuracy(our_second_classifier,movies_test2)\n",
    "improvement = accuracy_of_student_classifier - baseline\n",
    "print(\"Accuracy of the student-defined classifier:\", 100*accuracy_of_student_classifier, \"%\")\n",
    "print(\"Baseline (accuracy of the classifier we defined above):\", 100*baseline, \"%\")\n",
    "print(\"Improvement compared with baseline:\", round(100*improvement, 5), \"%\")\n",
    "student_defined_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e0c1a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "               fantastic = True              pos : neg    =      4.4 : 1.0\n",
      "                   worse = True              neg : pos    =      2.2 : 1.0\n",
      "                    poor = True              neg : pos    =      2.1 : 1.0\n",
      "                     bad = True              neg : pos    =      1.9 : 1.0\n",
      "              incredible = True              pos : neg    =      1.8 : 1.0\n",
      "                     bad = False             pos : neg    =      1.5 : 1.0\n",
      "                   enjoy = True              pos : neg    =      1.4 : 1.0\n",
      "                    like = False             pos : neg    =      1.2 : 1.0\n",
      "                   weird = True              neg : pos    =      1.2 : 1.0\n",
      "                    love = True              pos : neg    =      1.2 : 1.0\n",
      "                     fan = True              neg : pos    =      1.1 : 1.0\n",
      "                    love = False             neg : pos    =      1.1 : 1.0\n",
      "                   worse = False             pos : neg    =      1.1 : 1.0\n",
      "                    like = True              neg : pos    =      1.1 : 1.0\n",
      "                    poor = False             pos : neg    =      1.1 : 1.0\n",
      "               fantastic = False             neg : pos    =      1.0 : 1.0\n",
      "                   enjoy = False             neg : pos    =      1.0 : 1.0\n",
      "                    good = False             neg : pos    =      1.0 : 1.0\n",
      "              incredible = False             neg : pos    =      1.0 : 1.0\n",
      "                    good = True              pos : neg    =      1.0 : 1.0\n",
      "                   weird = False             pos : neg    =      1.0 : 1.0\n",
      "                     fan = False             pos : neg    =      1.0 : 1.0\n",
      "               displease = False             neg : pos    =      1.0 : 1.0\n",
      "               pleassant = False             neg : pos    =      1.0 : 1.0\n",
      "               top-notch = False             neg : pos    =      1.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# show most informative features of second classifier\n",
    "our_second_classifier.show_most_informative_features(len(student_defined_features)*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1a56da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n"
     ]
    }
   ],
   "source": [
    "# print the amount of improvement you made over the original classifier\n",
    "print(round(improvement, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f748de",
   "metadata": {},
   "source": [
    "I chose these words because they had either a strong positive or negative connotations. Outside of the context of the reviews, I thought that these woirds were far from being neutral. I think that the reason why I only had a 1% improvement over the baseline accuracy is because some of these features are not really common which would affect the odds ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6341add",
   "metadata": {},
   "source": [
    "### 2.7. Assessing Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff7cc23",
   "metadata": {},
   "source": [
    "Imagine a user whose job is to promote a movie and who is using our classifier to automatically identify positive reviews in order to use them in advertising materials. For this user, negative reviews are irrelevant because the user is not interested in any negative reviews the system might be identifying. (In other words, positive reviews are like ham and negative reviews are like spam. Or to use another analogy, positive reviews are like relevant search engine results and negative reviews are like irrelevant search engine results. We are using this conceit so that we can apply the notions of precision and recall to this situation.) Let's assess the classifier's performance in terms of precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "039cabea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 . pos\n",
      "2 . neg\n",
      "3 . pos\n",
      "4 . neg\n",
      "5 . pos\n",
      "6 . neg\n",
      "7 . neg\n",
      "8 . pos\n",
      "9 . pos\n",
      "10 . neg\n"
     ]
    }
   ],
   "source": [
    "# classify the reviews given above using the classifier that has been trained on the features defined above\n",
    "i = 1\n",
    "for review in new_test_set:\n",
    "    print(i,'.',our_second_classifier.classify(extract_features(nltk.word_tokenize(review),student_defined_features)))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c265236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the precision, recall, and overall accuracy (defined as the proportion of reviews that were correctly classified) of the classifier as a number between zero and one\n",
    "\n",
    "counter=0\n",
    "\n",
    "pos_classified_as_pos=0.0 # true positive -- a result that is correctly classified as positive\n",
    "pos_classified_as_neg=0.0 # false negative -- a positive result that is incorrectly classified as negative\n",
    "neg_classified_as_pos=0.0 # false positive -- a negative result that is incorrectly classified as positive\n",
    "neg_classified_as_neg=0.0 # true negative -- a result that is correctly classified as negative\n",
    "\n",
    "# compute numbers of true and false positives and negatives\n",
    "for review in new_test_set:\n",
    "    what_classifier_thinks = our_second_classifier.classify(extract_features(nltk.word_tokenize(review),student_defined_features))\n",
    "    the_truth = true_labels[counter]\n",
    "    if the_truth == 'pos' and what_classifier_thinks == 'pos':\n",
    "        pos_classified_as_pos+=1 # true positive\n",
    "    if  the_truth == 'pos' and what_classifier_thinks == 'neg':\n",
    "        pos_classified_as_neg+=1 # false negative\n",
    "    if the_truth == 'neg' and what_classifier_thinks == 'pos':\n",
    "        neg_classified_as_pos+=1 # false positive\n",
    "    if the_truth == 'neg' and what_classifier_thinks == 'neg':\n",
    "        neg_classified_as_neg+=1 # true negative\n",
    "    counter += 1 # increase counter by 1\n",
    "\n",
    "total = counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ebf0041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correctly classified reviews: 8.0\n"
     ]
    }
   ],
   "source": [
    "correctly_classified_reviews = pos_classified_as_pos+neg_classified_as_neg\n",
    "print(\"Number of correctly classified reviews:\", correctly_classified_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e365b55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of incorrectly classified reviews: 2.0\n"
     ]
    }
   ],
   "source": [
    "incorrectly_classified_reviews = pos_classified_as_neg+neg_classified_as_pos\n",
    "print(\"Number of incorrectly classified reviews:\", incorrectly_classified_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c802d7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correctly classified reviews overall: 8\n",
      "Number of incorrectly classified reviews overall: 2\n",
      "Number of positive reviews correctly classified as positive: 4\n",
      "Number of positive reviews incorrectly classified as negative: 1\n",
      "Number of negative reviews incorrectly classified as positive: 1\n",
      "Number of negative reviews correctly classified as negative: 4\n"
     ]
    }
   ],
   "source": [
    "def print_results():\n",
    "    print(\"Number of correctly classified reviews overall:\", int(correctly_classified_reviews))\n",
    "    print(\"Number of incorrectly classified reviews overall:\", int(incorrectly_classified_reviews))\n",
    "    print(\"Number of positive reviews correctly classified as positive:\", int(pos_classified_as_pos))\n",
    "    print(\"Number of positive reviews incorrectly classified as negative:\", int(pos_classified_as_neg))\n",
    "    print(\"Number of negative reviews incorrectly classified as positive:\", int(neg_classified_as_pos))\n",
    "    print(\"Number of negative reviews correctly classified as negative:\", int(neg_classified_as_neg))\n",
    "print_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3763826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision as a number between zero and one: 0.8\n",
      "Precision in percent: 80.0\n"
     ]
    }
   ],
   "source": [
    "precision = pos_classified_as_pos/(pos_classified_as_pos+neg_classified_as_pos)\n",
    "print(\"Precision as a number between zero and one:\", precision)\n",
    "print(\"Precision in percent:\", precision*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ccbf279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall as a number between zero and one: 0.8\n",
      "Recall in percent: 80.0\n"
     ]
    }
   ],
   "source": [
    "recall = pos_classified_as_pos/(pos_classified_as_pos+pos_classified_as_neg)\n",
    "print(\"Recall as a number between zero and one:\", recall)\n",
    "print(\"Recall in percent:\", recall*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ae4e4dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy as a number between zero and one: 0.8\n",
      "Accuracy in percent: 80.0\n"
     ]
    }
   ],
   "source": [
    "accuracy = correctly_classified_reviews/(correctly_classified_reviews+incorrectly_classified_reviews)\n",
    "print(\"Accuracy as a number between zero and one:\", accuracy)\n",
    "print(\"Accuracy in percent:\", accuracy*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:otter-env]",
   "language": "python",
   "name": "conda-env-otter-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
